---
title: "Exercise 12 - Misc Text and Networks"
author: "Ken Benoit"
output: html_document
---

1.  **Using regular expressions**
    
    Regular expressions are very important concepts in text processing, as they offer
    tools for searching and matching text strings based on symbolic representations.
    For the dictionary and thesaurus features, we can define equivalency classes in terms
    of regular expressions.  There is an excellent tutorial on regular expressions at
    http://www.regular-expressions.info.
    
    This provides an easy way to recover syntactic variations on specific words, without relying
    on a stemmer.  For instance, we could query a regular expression on tax-related words, using: 
    ```{r, eval = FALSE}
    require(quanteda)
    kwic(data_corpus_inaugural, "tax", valuetype = "regex")
    kwic(data_corpus_inaugural, "tax")
    ```
    
    What is the result between that command, and the version `kwic(data_corpus_inaugural, "tax")`?  
    
    What if we on wanted to construct a regular expression to query only "valued" and "values" but not other variations of the lemma "value"?
    ```{r}
    kwic(data_corpus_inaugural, "value[ds]", valuetype = "regex")
    ```

    Could we construct a "glob" pattern match for the same two words?
    
2.  **Continuing with dictionaries**

    Dictionaries may be hierarchical, where a top-level key can consist of subordinate keys, 
    each a list of its own.  For instance, 
    `list(articles = list(definite="the", indefinite=c("a", "and"))` defines a valid
    list for articles.  Make a dictionary of articles and conjunctions where you define two
    levels, one for definite and indefinite articles, and one for coordinating and 
    subordinating conjunctions.  (A sufficient list for your purposes of  subordinating 
    conjuctions is "although", "because", "since", "unless".)
    
    ```{r}
    hDict <- dictionary(list(articles = list(definite = "the", indefinite = c("a", "an")),
                             conjuctions = list(coordinating = c("and", "but", "or", "nor", "for", "yet", "so"),
                                                subordinating = c("although", "because", "since", "unless"))))
    hDict
    ```
    
    Now apply this to the `inaugCorpus` object, and examine the resulting features.  What happened to the hierarchies, to make them
    into "features"?  Do the subcategories sum to the two categories from the previous question?
    

    ```{r}
    inaugCorpus <- corpus(data_corpus_inaugural, text_field = speeches)
    inaugDfm <- dfm(inaugCorpus, dict = hDict)
    head(inaugDfm)
    ```
            
3.  **Getting used to thesauruses**

    A "thesaurus" is a list of feature equivalencies specified in the same list format as a dictionary, 
    but which---unlike a dictionary---returns all the features *not* specified as entries in the
    thesaurus.  
    
    If we wanted to count pronouns as equivalent, for instance, we could use the thesaurus argument
    to `dfm` in order to group all listed prounouns into a single feature labelled "PRONOUN".
    
    ```{r}
    require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
    mytexts <- c("We are not schizophrenic, but I am.", "I bought myself a new car.")
    myThes <- dictionary(list(pronouns = list(firstp = c("I", "me", "my", "mine", "myself", "we", "us", "our", "ours"))))
    myDfm <- dfm(mytexts, thesaurus = myThes)
    myDfm
    ```
    
    Notice how the thesaurus key has been made into uppercase --- this is to identify it as 
    a key, as opposed to a word feature from the original text.
    
    Try running the articles and conjunctions dictionary from the previous exercise on 
    as a thesaurus, and compare the results.
    
    ```{r}
    dfm(mytexts, thesaurus = hDict)
    ```
    
4.  Keyword analysis

    a) After aggregating by party for all US presidents since 1960, perform a keyword analysis to identity the top 10 Democrat and top 10 Republican keywords.
    
    ```{r}
    data("data_corpus_sotu", package = "quanteda.corpora")
    docvars(data_corpus_sotu, "year") <- as.numeric(format(docvars(data_corpus_sotu, "Date"), "%Y"))
    
    sotu_1960 <- corpus_subset(data_corpus_sotu, year >= 1960)
    
    sotu_dfm <- dfm(sotu_1960,
                    groups = "party", 
                    remove_punct = TRUE,
                    remove_numbers = TRUE, 
                    remove = stopwords("english"))
    
    sotu_dfm[, 1:10]
    
    twoparty_key <- textstat_keyness(sotu_dfm, target = "Democratic")
    twoparty_key
    ```
    
    b) Plot this using a keyness plot.
    
    ```{r}
    textplot_keyness(twoparty_key)
    ```
    
    **Hint: See the examples from `?textplot_keyness`.
    
5.  Network analysis

    For the same texts from (3) above, plot a network for the terms with a minimum frequency of 0.8.
    
    ```{r}
    toks <- sotu_1960 %>%
      tokens(remove_punct = TRUE) %>%
      tokens_tolower() %>%
      tokens_remove(stopwords("english"), padding = FALSE)
    
    myfcm <- fcm(toks, context = "window", tri = FALSE)
    feat <- names(topfeatures(myfcm, 30))
    fcm_select(myfcm, feat, verbose = FALSE) %>%
      textplot_network(min_freq = 0.8)
    ```
    