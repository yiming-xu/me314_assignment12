---
title: "Exercise 12 - Misc Text and Networks"
author: "Ken Benoit"
output: html_document
---

1.  **Using regular expressions**
    
    Regular expressions are very important concepts in text processing, as they offer
    tools for searching and matching text strings based on symbolic representations.
    For the dictionary and thesaurus features, we can define equivalency classes in terms
    of regular expressions.  There is an excellent tutorial on regular expressions at
    http://www.regular-expressions.info.
    
    This provides an easy way to recover syntactic variations on specific words, without relying
    on a stemmer.  For instance, we could query a regular expression on tax-related words, using: 
    ```{r}
    require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
    kwic(data_corpus_inaugural, "tax", window = 2, valuetype = "regex")
    ```
    
    What is the result between that command, and the version `kwic(inaugCorpus, "tax")`?  
    ```{r}
    kwic(data_corpus_inaugural, "tax", window = 2)
    ```
    **The second version returns only the word "tax" as a whole word, not "tax" as a regular expression, which would match to "taxes", "stax", "tiktax", etc.**
    
    What if we on wanted to construct a regular expression to query only "valued" and "values" but not other variations of the lemma "value"?
    ```{r}
    kwic(data_corpus_inaugural, "^value[ds]$", window = 2, valuetype = "regex")
    ```

    Could we construct a "glob" pattern match for the same two words?
    ```{r}
    kwic(data_corpus_inaugural, "value?", 2)
    ```
    **No, because the `?` matches any single character.  But here we got lucky, since there were no other words with a single letter following the string `value`.  Had "valuey" (for example) been present in the corpus, the glob search would have returned it.**
    
2.  **Continuing with dictionaries**
        
    ```{r}
    hDict <- dictionary(list(articles = list(definite = "the", indefinite = c("a", "an")),
                             conjuctions = list(coordinating = c("and", "but", "or", "nor", "for", "yet", "so"),
                                                subordinating = c("although", "because", "since", "unless"))))
    hDict
    ```
        
    Now apply this to the `inaugCorpus` object, and examine the resulting features.  What happened to the hierarchies, to make them
    into "features"?  Do the subcategories sum to the two categories from the previous question?
    
    ```{r}
    inaugHDictDfm <- dfm(inaugCorpus, dictionary = hDict)
    head(inaugHDictDfm)
    ```
    **quanteda "flattened" the hierarchical dictionary prior to applying it to the text.**
        
        
3.  **Getting used to thesauruses**

    A "thesaurus" is a list of feature equivalencies specified in the same list format as a dictionary, 
    but which---unlike a dictionary---returns all the features *not* specified as entries in the
    thesaurus.  
    
    If we wanted to count pronouns as equivalent, for instance, we could use the thesaurus argument
    to `dfm` in order to group all listed prounouns into a single feature labelled "PRONOUN".
    ```{r}
    mytexts <- c("We are not schizophrenic, but I am.", "I bought myself a new car.")
    myThes <- dictionary(list(pronouns = list(firstp=c("I", "me", "my", "mine", "myself", "we", "us", "our", "ours"))))
    myDfm <- dfm(mytexts, thesaurus = myThes)
    myDfm
    ```
    Notice how the thesaurus key has been made into uppercase --- this is to identify it as 
    a key, as opposed to a word feature from the original text.
    
    Try running the articles and conjunctions dictionary from the previous exercise on 
    as a thesaurus, and compare the results.
    
    ```{r}
    posDfm <- dfm(inaugCorpus, thesaurus = posDict)
    relativeDfm <- tf(posDfm, "prop")
    Year <- as.integer(docvars(inaugCorpus, "Year"))
    plot(Year, relativeDfm[, "ARTICLES"], 
         ylim = c(0.0, 0.14),          # fix the range so that both sets fit
         pch = 19, col = "blue",  # blue solid points
         type = "b",
         ylab = "Relative Proportion of All Tokens")
    text(1950, .12, "articles")
    points(Year, relativeDfm[, "CONJUNCTIONS"], pch = 19, col = "grey30", type = "b")
    text(1850, .04, "conjuctions")
    abline(h = colMeans(relativeDfm[, c("ARTICLES", "CONJUNCTIONS")]), col = c("blue", "grey30"), lty = "dotted")
    ```

4.  Keyword analysis

    a) After aggregating by party for all US presidents since 1960, perform a keyword analysis to identity the top 10 Democrat and top 10 Republican keywords.
    
```{r}
data("data_corpus_sotu", package = "quanteda.corpora")
docvars(data_corpus_sotu, "year") <- as.numeric(format(docvars(data_corpus_sotu, "Date"), "%Y"))

sotu_1960 <- corpus_subset(data_corpus_sotu, year >= 1960)

sotu_dfm <- dfm(sotu_1960, groups = "party", remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))

sotu_dfm[, 1:10]

twoparty_key <- textstat_keyness(sotu_dfm, target = "Democratic")
twoparty_key
```

    
    b) Plot this using a keyness plot.
    
```{r}
textplot_keyness(twoparty_key)
```
    
    
    **Hint: See the examples from `?textplot_keyness`.
    
5.  Network analysis

    For the same texts from (3) above, plot a network for the terms with a minimum frequency of 0.8.

```{r}
toks <- sotu_1960 %>%
  tokens(remove_punct = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("english"), padding = FALSE)
myfcm <- fcm(toks, context = "window", tri = FALSE)
feat <- names(topfeatures(myfcm, 30))
fcm_select(myfcm, feat, verbose = FALSE) %>%
  textplot_network(min_freq = 0.8)
```

